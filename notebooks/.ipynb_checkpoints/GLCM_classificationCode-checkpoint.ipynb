{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd499f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import imageio\n",
    "from skimage import feature\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import glob\n",
    "import collections\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ab701",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = np.load('../patches/all_120/X_train_120_120_all.npz')\n",
    "X_train_file.files\n",
    "X_train_orignal = X_train_file['arr_0']\n",
    "\n",
    "Y_train_file = np.load('../patches/all_120/Y_train_120_120_all.npz')\n",
    "Y_train_file.files\n",
    "Y_train_orignal = Y_train_file['arr_0']\n",
    "\n",
    "X_test_file = np.load('../patches/all_120/X_test_120_120_all.npz')\n",
    "X_test_file.files\n",
    "X_test_orignal = X_test_file['arr_0']\n",
    "\n",
    "Y_test_file = np.load('../patches/all_120/Y_test_120_120_all.npz')\n",
    "Y_test_file.files\n",
    "Y_test_orignal = Y_test_file['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = collections.Counter(Y_train_orignal)\n",
    "print(dict(frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a777db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute some GLCM properties each patch\n",
    "diss_sim = []\n",
    "corr = []\n",
    "homogen = []\n",
    "energy = []\n",
    "contrast = []\n",
    "for patch in X_train_orignal:\n",
    "    glcm = greycomatrix(patch, distances=[5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256)\n",
    "    diss_sim.append(greycoprops(glcm, 'dissimilarity')[0, 0]) #[0,0] to convert array to value\n",
    "    corr.append(greycoprops(glcm, 'correlation')[0, 0])\n",
    "    homogen.append(greycoprops(glcm, 'homogeneity')[0, 0])\n",
    "    energy.append(greycoprops(glcm, 'energy')[0, 0])\n",
    "    contrast.append(greycoprops(glcm, 'contrast')[0, 0])\n",
    "\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "column_names = [\"dissimilarity\", \"correlation\",\"homogeneity\", \"energy\",\"contrast\"]\n",
    "rocks_DF_size120_train=pd.DataFrame(columns = column_names)\n",
    "\n",
    "rocks_DF_size120_train['dissimilarity']=diss_sim\n",
    "rocks_DF_size120_train['correlation']=corr\n",
    "rocks_DF_size120_train['homogeneity']=homogen\n",
    "rocks_DF_size120_train['energy']=energy\n",
    "rocks_DF_size120_train['contrast']=contrast\n",
    "\n",
    "display(rocks_DF_size120_train)\n",
    "\n",
    "# compute some GLCM properties each patch\n",
    "diss_sim = []\n",
    "corr = []\n",
    "homogen = []\n",
    "energy = []\n",
    "contrast = []\n",
    "for patch in X_test_orignal:\n",
    "    glcm = greycomatrix(patch, distances=[5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256)\n",
    "    diss_sim.append(greycoprops(glcm, 'dissimilarity')[0, 0]) #[0,0] to convert array to value\n",
    "    corr.append(greycoprops(glcm, 'correlation')[0, 0])\n",
    "    homogen.append(greycoprops(glcm, 'homogeneity')[0, 0])\n",
    "    energy.append(greycoprops(glcm, 'energy')[0, 0])\n",
    "    contrast.append(greycoprops(glcm, 'contrast')[0, 0])\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "column_names = [\"dissimilarity\", \"correlation\",\"homogeneity\", \"energy\",\"contrast\"]\n",
    "rocks_DF_size120_test=pd.DataFrame(columns = column_names)\n",
    "\n",
    "rocks_DF_size120_test['dissimilarity']=diss_sim\n",
    "rocks_DF_size120_test['correlation']=corr\n",
    "rocks_DF_size120_test['homogeneity']=homogen\n",
    "rocks_DF_size120_test['energy']=energy\n",
    "rocks_DF_size120_test['contrast']=contrast\n",
    "\n",
    "display(rocks_DF_size120_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55208b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocks_DF_size120_train = rocks_DF_size120_train.copy()\n",
    "for column in rocks_DF_size120_train.columns:\n",
    "    rocks_DF_size120_train[column] = (rocks_DF_size120_train[column]  - np.min(rocks_DF_size120_train[column]))/(np.max(rocks_DF_size120_train[column]) - np.min(rocks_DF_size120_train[column]))\n",
    "display(rocks_DF_size120_train.head)\n",
    "\n",
    "\n",
    "rocks_DF_size120_test = rocks_DF_size120_test.copy()\n",
    "for column in rocks_DF_size120_test.columns:\n",
    "    rocks_DF_size120_test[column] = (rocks_DF_size120_test[column]  - np.min(rocks_DF_size120_test[column]))/(np.max(rocks_DF_size120_test[column]) - np.min(rocks_DF_size120_test[column]))\n",
    "display(rocks_DF_size120_test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "rocks_DF_size120_train, Y_train_orignal = oversample.fit_resample(rocks_DF_size120_train, Y_train_orignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "acc=[]\n",
    "for fold in range(0,10):\n",
    "    d_train = lgb.Dataset(rocks_DF_size120_train, label=Y_train_orignal)\n",
    "\n",
    "    lgbm_params = {'learning_rate':0.9, 'boosting_type':'gbdt',    \n",
    "                  'objective':'multiclass',\n",
    "                  'metric': 'multi_logloss',\n",
    "                  'num_class':53,\n",
    "                  'device':'gpu'}\n",
    "\n",
    "\n",
    "    lgb_model = lgb.train(lgbm_params, d_train, 10)\n",
    "\n",
    "    #Predict on test\n",
    "    test_prediction = lgb_model.predict(rocks_DF_size120_test)\n",
    "    test_prediction=np.argmax(test_prediction, axis=1)\n",
    "    acc.append(metrics.accuracy_score(Y_test_orignal, test_prediction)*100)\n",
    "    \n",
    "finalAccuracy = np.max(acc)\n",
    "plt.plot(acc)\n",
    "print (\"Accuracy = \", finalAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(rocks_DF_size120_train, Y_train_orignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set evaluation\n",
    "prediction = clf.predict(rocks_DF_size120_test)\n",
    "# visualization\n",
    "cmat = metrics.confusion_matrix(prediction,Y_test_orignal)\n",
    "s = sns.heatmap(cmat,annot=True)\n",
    "s.set(xlabel='Predicted', ylabel='Actual')\n",
    "acc = cmat.trace() / cmat.sum()\n",
    "print('Accuracy: {0:5.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=[]\n",
    "for fold in range(1,1500,50): \n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=fold,algorithm='kd_tree',n_jobs=8)\n",
    "    clf.fit(rocks_DF_size120_train, Y_train_orignal)\n",
    "    y_pred = clf.predict(rocks_DF_size120_test)\n",
    "    print (\"Accuracy = \", metrics.accuracy_score(Y_test_orignal, y_pred)*100)\n",
    "    acc.append(metrics.accuracy_score(Y_test_orignal, y_pred)*100)\n",
    "    \n",
    "plt.plot(acc)\n",
    "acc=np.max(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9818c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = RandomForestClassifier(n_estimators=100,min_samples_split=4,bootstrap=True,n_jobs=8,verbose=4)\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, rocks_DF_size120_train, Y_train_orignal, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "plt.figure()\n",
    "plt.plot(n_scores)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29206a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(rocks_DF_size120_train, Y_train_orignal)\n",
    "\n",
    "y_pred = model.predict(rocks_DF_size120_test)\n",
    "print (\"Accuracy = \", metrics.accuracy_score(Y_test_orignal, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
